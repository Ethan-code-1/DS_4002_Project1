{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use: \n",
    "\n",
    "1.) Load the Movie & TV meta data and review data json files from https://amazon-reviews-2023.github.io\n",
    "\n",
    "2.) With those folders in the same directory as this script, run all cells. This will randomly remove 99% of movies and their associated reviews to make the file size more managable. Estimated local processing time ~7 minutes. Console messages will update the user with progress periodically. \n",
    "\n",
    "\n",
    "*If you want to use the already reduced json (recommended) see the data folder. We could not include the json file that this script operates on because its size is substantially larger than github Repo limits and will require substantial download and processing time.\n",
    "\n",
    "Credit: Script development assisted by ChatGPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting unique parent_asins...\n",
      "Processed 1,000,000 reviews...\n",
      "Processed 2,000,000 reviews...\n",
      "Processed 3,000,000 reviews...\n",
      "Processed 4,000,000 reviews...\n",
      "Processed 5,000,000 reviews...\n",
      "Processed 6,000,000 reviews...\n",
      "Processed 7,000,000 reviews...\n",
      "Processed 8,000,000 reviews...\n",
      "Processed 9,000,000 reviews...\n",
      "Processed 10,000,000 reviews...\n",
      "Processed 11,000,000 reviews...\n",
      "Processed 12,000,000 reviews...\n",
      "Processed 13,000,000 reviews...\n",
      "Processed 14,000,000 reviews...\n",
      "Processed 15,000,000 reviews...\n",
      "Processed 16,000,000 reviews...\n",
      "Processed 17,000,000 reviews...\n",
      "Total reviews: 17,328,314\n",
      "Total unique parent_asins: 747,764\n",
      "Selected 7,477 parent_asins (~1%)\n",
      "Filtering and writing selected reviews...\n",
      "Scanned 500,000 reviews, written 4,588 so far...\n",
      "Scanned 1,000,000 reviews, written 9,038 so far...\n",
      "Scanned 1,500,000 reviews, written 13,570 so far...\n",
      "Scanned 2,000,000 reviews, written 18,123 so far...\n",
      "Scanned 2,500,000 reviews, written 22,683 so far...\n",
      "Scanned 3,000,000 reviews, written 27,507 so far...\n",
      "Scanned 3,500,000 reviews, written 31,848 so far...\n",
      "Scanned 4,000,000 reviews, written 35,942 so far...\n",
      "Scanned 4,500,000 reviews, written 40,431 so far...\n",
      "Scanned 5,000,000 reviews, written 45,036 so far...\n",
      "Scanned 5,500,000 reviews, written 49,414 so far...\n",
      "Scanned 6,000,000 reviews, written 54,474 so far...\n",
      "Scanned 6,500,000 reviews, written 58,700 so far...\n",
      "Scanned 7,000,000 reviews, written 63,302 so far...\n",
      "Scanned 7,500,000 reviews, written 67,477 so far...\n",
      "Scanned 8,000,000 reviews, written 72,751 so far...\n",
      "Scanned 8,500,000 reviews, written 76,946 so far...\n",
      "Scanned 9,000,000 reviews, written 81,750 so far...\n",
      "Scanned 9,500,000 reviews, written 86,012 so far...\n",
      "Scanned 10,000,000 reviews, written 90,575 so far...\n",
      "Scanned 10,500,000 reviews, written 94,730 so far...\n",
      "Scanned 11,000,000 reviews, written 98,892 so far...\n",
      "Scanned 11,500,000 reviews, written 103,188 so far...\n",
      "Scanned 12,000,000 reviews, written 107,429 so far...\n",
      "Scanned 12,500,000 reviews, written 112,385 so far...\n",
      "Scanned 13,000,000 reviews, written 116,766 so far...\n",
      "Scanned 13,500,000 reviews, written 120,932 so far...\n",
      "Scanned 14,000,000 reviews, written 125,960 so far...\n",
      "Scanned 14,500,000 reviews, written 130,143 so far...\n",
      "Scanned 15,000,000 reviews, written 134,081 so far...\n",
      "Scanned 15,500,000 reviews, written 138,239 so far...\n",
      "Scanned 16,000,000 reviews, written 143,165 so far...\n",
      "Scanned 16,500,000 reviews, written 147,363 so far...\n",
      "Scanned 17,000,000 reviews, written 151,457 so far...\n",
      "Final written reviews: 154,643\n",
      "Reduced dataset saved to All_Movies_tv_reduced.jsonl\n",
      "Filtering and overwriting the reduced dataset...\n",
      "Finished processing and overwriting All_Movies_tv_reduced.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Input and output file names\n",
    "file = \"Movies_and_TV.jsonl\"  # Original dataset\n",
    "reduced_file = \"All_Movies_tv_reduced.jsonl\"  # First-stage filtered dataset\n",
    "\n",
    "# Step 1: Count unique parent_asins without storing reviews\n",
    "parent_asin_counts = {}\n",
    "total_reviews = 0\n",
    "\n",
    "print(\"Counting unique parent_asins...\")\n",
    "\n",
    "with open(file, 'r') as fp:\n",
    "    for i, line in enumerate(fp, 1):\n",
    "        review = json.loads(line.strip())\n",
    "\n",
    "        # Ensure parent_asin exists\n",
    "        if \"parent_asin\" in review:\n",
    "            parent_asin = review[\"parent_asin\"]\n",
    "            parent_asin_counts[parent_asin] = parent_asin_counts.get(parent_asin, 0) + 1\n",
    "\n",
    "        total_reviews += 1\n",
    "\n",
    "        # Print progress every 1 million reviews\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"Processed {i:,} reviews...\")\n",
    "\n",
    "print(f\"Total reviews: {total_reviews:,}\")\n",
    "print(f\"Total unique parent_asins: {len(parent_asin_counts):,}\")\n",
    "\n",
    "# Step 2: Randomly select 10% of parent_asins\n",
    "num_selected = int(0.01 * len(parent_asin_counts))\n",
    "selected_parents = set(random.sample(list(parent_asin_counts.keys()), num_selected))\n",
    "\n",
    "print(f\"Selected {num_selected:,} parent_asins (~1%)\")\n",
    "\n",
    "# Step 3: Stream through the file again and write only selected reviews\n",
    "print(\"Filtering and writing selected reviews...\")\n",
    "written_reviews = 0\n",
    "\n",
    "with open(file, 'r') as fp, open(reduced_file, 'w') as out_fp:\n",
    "    for i, line in enumerate(fp, 1):\n",
    "        review = json.loads(line.strip())\n",
    "\n",
    "        # Ensure parent_asin exists and is selected\n",
    "        if review.get(\"parent_asin\") in selected_parents:\n",
    "            out_fp.write(json.dumps(review) + \"\\n\")\n",
    "            written_reviews += 1\n",
    "\n",
    "        # Print progress every 500,000 reviews\n",
    "        if i % 500_000 == 0:\n",
    "            print(f\"Scanned {i:,} reviews, written {written_reviews:,} so far...\")\n",
    "\n",
    "print(f\"Final written reviews: {written_reviews:,}\")\n",
    "print(f\"Reduced dataset saved to {reduced_file}\")\n",
    "\n",
    "# Step 4: Overwrite the reduced file to keep only necessary fields\n",
    "print(\"Filtering and overwriting the reduced dataset...\")\n",
    "\n",
    "temp_file = \"temp_filtered.jsonl\"  # Temporary file for safe writing\n",
    "\n",
    "with open(reduced_file, 'r') as fp, open(temp_file, 'w') as out_fp:\n",
    "    for i, line in enumerate(fp, 1):\n",
    "        review = json.loads(line.strip())\n",
    "\n",
    "        # Ensure required fields exist before writing\n",
    "        filtered_review = {\n",
    "            \"rating\": review.get(\"rating\"),\n",
    "            \"title\": review.get(\"title\"),\n",
    "            \"text\": review.get(\"text\"),\n",
    "            \"parent_asin\": review.get(\"parent_asin\")\n",
    "        }\n",
    "\n",
    "        # Only write valid reviews (skip if missing required fields)\n",
    "        if all(filtered_review.values()):\n",
    "            out_fp.write(json.dumps(filtered_review) + \"\\n\")\n",
    "\n",
    "        # Print progress every 500,000 reviews\n",
    "        if i % 500_000 == 0:\n",
    "            print(f\"Processed {i:,} reviews...\")\n",
    "\n",
    "# Replace original file with the filtered version\n",
    "import os\n",
    "os.replace(temp_file, reduced_file)\n",
    "\n",
    "print(f\"Finished processing and overwriting {reduced_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parent_asins from reduced reviews...\n",
      "Total unique parent_asins in reduced reviews: 7,477\n",
      "Filtering movie metadata...\n",
      "Scanned 500,000 movies, kept 5,037 so far...\n",
      "Final kept movies: 7,477\n",
      "Filtered movie metadata saved to movies_metadata_reduced.jsonl\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "reviews_file = \"All_Movies_tv_reduced.jsonl\"  # The reduced review dataset\n",
    "movies_file = \"meta_Movies_and_TV.jsonl\"  # Original movie metadata file\n",
    "output_file = \"movies_metadata_reduced.jsonl\"  # Final filtered movie metadata\n",
    "\n",
    "# Step 1: Load the set of parent_asins that exist in the reduced reviews dataset\n",
    "print(\"Loading parent_asins from reduced reviews...\")\n",
    "\n",
    "valid_parent_asins = set()\n",
    "with open(reviews_file, 'r') as fp:\n",
    "    for i, line in enumerate(fp, 1):\n",
    "        review = json.loads(line.strip())\n",
    "        if \"parent_asin\" in review:\n",
    "            valid_parent_asins.add(review[\"parent_asin\"])\n",
    "\n",
    "        # Print progress every 1 million reviews\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"Processed {i:,} reviews...\")\n",
    "\n",
    "print(f\"Total unique parent_asins in reduced reviews: {len(valid_parent_asins):,}\")\n",
    "\n",
    "# Step 2: Filter movie metadata based on valid parent_asins\n",
    "print(\"Filtering movie metadata...\")\n",
    "kept_movies = 0\n",
    "\n",
    "with open(movies_file, 'r') as fp, open(output_file, 'w') as out_fp:\n",
    "    for i, line in enumerate(fp, 1):\n",
    "        movie = json.loads(line.strip())\n",
    "\n",
    "        # Keep only movies with parent_asin in valid_parent_asins\n",
    "        if movie.get(\"parent_asin\") in valid_parent_asins:\n",
    "            out_fp.write(json.dumps(movie) + \"\\n\")\n",
    "            kept_movies += 1\n",
    "\n",
    "        # Print progress every 500,000 movies\n",
    "        if i % 500_000 == 0:\n",
    "            print(f\"Scanned {i:,} movies, kept {kept_movies:,} so far...\")\n",
    "\n",
    "print(f\"Final kept movies: {kept_movies:,}\")\n",
    "print(f\"Filtered movie metadata saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88309f64b095d5ce3915af14323263fe7bafe3d638d2d7c64b7b949ff436de30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
